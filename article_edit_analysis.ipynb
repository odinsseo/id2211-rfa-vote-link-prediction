{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97771fa",
   "metadata": {},
   "source": [
    "# Wikipedia Article Talk Analysis\n",
    "\n",
    "This notebook analyzes interactions in Wikipedia article talk pages, focusing on user discussions and contributions. The analysis includes:\n",
    "\n",
    "1. Processing and cleaning Wikipedia talk page data\n",
    "2. Analyzing user interactions and contribution patterns\n",
    "3. Building social and activity graphs from the data\n",
    "4. Examining temporal patterns in user activities\n",
    "\n",
    "The analysis uses data from the [wiki-meta dataset](https://snap.stanford.edu/data/wiki-meta.html), which includes both user-talk and article-talk interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad67129",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, we import the required libraries:\n",
    "- `pandas` and `numpy` for data manipulation\n",
    "- `seaborn` and `matplotlib` for visualization\n",
    "- `networkx` for graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fadf342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyvis.network import Network\n",
    "from tqdm.notebook import tqdm\n",
    "from networkx import bipartite\n",
    "import itertools\n",
    "from dask import dataframe as dd\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"whitegrid\",\n",
    "    palette=\"colorblind\",\n",
    "    rc={\"text.usetex\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61173ee",
   "metadata": {},
   "source": [
    "## Data Overview and Initial Processing\n",
    "\n",
    "### Dataset Statistics:\n",
    "\n",
    "1. **User-talk dataset**:\n",
    "   - Total entries: 11,629,578\n",
    "   - Bot-generated traffic: 1,600,129 (≈14%)\n",
    "   - Clean entries: 10,029,449\n",
    "   - Note: Self-links need to be removed as they don't represent inter-user interactions\n",
    "\n",
    "2. **Article-talk dataset**:\n",
    "   - Total entries: 13,384,702\n",
    "   - Bot-generated traffic:  1,511,005 (≈11%)\n",
    "   - Clean entries: 11,873,697"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a7b7f8",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We load the processed article talk data and elections data, converting timestamps to datetime format for temporal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4344a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986fc00881cf4e4382b543beaf9c6f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_article_edits():\n",
    "    article_data = []\n",
    "    for chunk in tqdm(pd.read_csv(\"data/article_edits.csv\", chunksize=1000000)):\n",
    "        chunk[\"timestamp\"] = (\n",
    "            pd.to_datetime(chunk[\"timestamp\"]).values.astype(np.int64) // 10**9\n",
    "        )\n",
    "        chunk[\"timestamp\"] = pd.to_datetime(chunk[\"timestamp\"], unit=\"s\")\n",
    "\n",
    "        chunk[\"minor\"] = chunk[\"minor\"].astype(bool)\n",
    "\n",
    "        article_data.append(chunk)\n",
    "\n",
    "    article_data = pd.concat(article_data, ignore_index=True)\n",
    "    article_data[\"user\"] = article_data[\"user\"].astype(\"category\")\n",
    "    article_data[\"namespace\"] = article_data[\"namespace\"].astype(\"category\")\n",
    "\n",
    "    return article_data\n",
    "\n",
    "article_edits = load_article_edits()\n",
    "\n",
    "voting_data = pd.read_csv(\"data/nominations_elections.csv\")\n",
    "voting_data[\"close_time\"] = pd.to_datetime(voting_data[\"close_time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3eae19",
   "metadata": {},
   "source": [
    "## Time-based Data Splitting\n",
    "\n",
    "We split the data into training and test sets based on temporal boundaries:\n",
    "- Validation cutoff: Last election before 2007-01-01\n",
    "- Test cutoff: Last election before 2007-12-01\n",
    "\n",
    "This temporal split helps evaluate our analysis on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b03aa8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_edits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m val_lim \u001b[38;5;241m=\u001b[39m voting_data[voting_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2007-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m      2\u001b[0m test_lim \u001b[38;5;241m=\u001b[39m voting_data[voting_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2007-12-01\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m----> 4\u001b[0m train: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[43marticle_edits\u001b[49m[article_edits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m test_lim]\n\u001b[0;32m      5\u001b[0m test: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m article_edits[article_edits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m test_lim]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'article_edits' is not defined"
     ]
    }
   ],
   "source": [
    "val_lim = voting_data[voting_data[\"close_time\"] < \"2007-01-01\"][\"close_time\"].max()\n",
    "test_lim = voting_data[voting_data[\"close_time\"] < \"2007-12-01\"][\"close_time\"].max()\n",
    "\n",
    "train: pd.DataFrame = article_edits[article_edits[\"timestamp\"] <= test_lim]\n",
    "test: pd.DataFrame = article_edits[article_edits[\"timestamp\"] > test_lim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37749ad0",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "### Distribution and Box Plot Function\n",
    "\n",
    "This function creates a combined view of data distribution:\n",
    "1. Histogram with density estimation (left)\n",
    "2. Box plot showing quartiles and outliers (right)\n",
    "\n",
    "Both plots use logarithmic scaling for better visualization of skewed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_and_boxplot(\n",
    "    data: pd.DataFrame | pd.Series, column: str | None, title: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a distribution plot and boxplot for a specified column in a DataFrame or Series.\n",
    "\n",
    "    Args:\n",
    "        data: pandas DataFrame or Series containing the data\n",
    "        column: name of the column to visualize (None if data is Series)\n",
    "        title: custom title/name for the plots\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    values = data[column] if isinstance(data, pd.DataFrame) else data\n",
    "\n",
    "    plt.subplot(121)\n",
    "    sns.histplot(data=values, bins=50, stat=\"density\", log_scale=(True, False))\n",
    "    plt.title(f\"Distribution of {title}\")\n",
    "    plt.xlabel(f\"{title} (log scale)\".capitalize())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    sns.boxplot(y=values)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(f\"Box plot of {title}\")\n",
    "    plt.ylabel(f\"{title} (log scale)\".capitalize())\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823248d",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "### Revision Word Count Analysis\n",
    "Analyzing the distribution of word counts in article talk page revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_and_boxplot(train, \"textdata\", \"revision word count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfde0a9",
   "metadata": {},
   "source": [
    "### Article Interaction Analysis\n",
    "Examining the distribution of interactions across different article namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55027a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = train.groupby([\"namespace\"]).size().reset_index(name=\"count\")\n",
    "plot_distribution_and_boxplot(articles, \"count\", \"number of article interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61aee7",
   "metadata": {},
   "source": [
    "### Election Timing Analysis\n",
    "Analyzing the temporal patterns of elections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = voting_data[\"close_time\"].sort_values()\n",
    "intervals = (intervals.diff().dt.total_seconds() / (24 * 60 * 60)).dropna()\n",
    "intervals = intervals.reset_index()\n",
    "plot_distribution_and_boxplot(\n",
    "    intervals, column=\"close_time\", title=\"time between elections (days)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671dac2e",
   "metadata": {},
   "source": [
    "## Graph Building Functions\n",
    "\n",
    "### Social Graph Construction\n",
    "Building user interaction networks based on talk page activities:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90da30",
   "metadata": {},
   "source": [
    "## Graph Weight Computation\n",
    "\n",
    "The weight of edges in our interaction graphs is computed using a formula that combines multiple factors to capture the significance and recency of user interactions. Here's the detailed breakdown of the formula:\n",
    "\n",
    "$$\n",
    "w_{u,a} = \\sum_{r\\in\\mathcal R_{u\\to a}} \\log(1+\\text{words}_r)\\,\\bigl(1-0.25\\,\\mathbb I\\{\\text{minor}_r\\}\\bigr)\\,e^{-\\lambda (t_\\text{cutoff}-t_r)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "1. $w_{u,a}$ is the total weight of interactions between user $u$ and article/namespace $a$\n",
    "2. $\\mathcal R_{u\\to a}$ represents all revisions by user $u$ to article/namespace $a$\n",
    "3. For each revision $r$:\n",
    "   - $\\text{words}_r$ is the word count of the revision\n",
    "   - $\\mathbb I\\{\\text{minor}_r\\}$ is an indicator function (1 if minor edit, 0 if major)\n",
    "   - $t_r$ is the timestamp of the revision\n",
    "   - $t_\\text{cutoff}$ is our analysis cutoff date\n",
    "   - $\\lambda$ is the decay parameter (set to $\\ln(2)/30$ for a 30-day half-life)\n",
    "\n",
    "### Components of the Formula:\n",
    "\n",
    "1. **Content Weight**: $\\log(1+\\text{words}_r)$\n",
    "   - Uses log-scale to prevent very long revisions from dominating\n",
    "   - Adding 1 prevents taking log(0) for empty revisions\n",
    "\n",
    "2. **Edit Type Weight**: $1-0.75\\,\\mathbb I\\{\\text{minor}_r\\}$\n",
    "   - Major edits have weight 1.0\n",
    "   - Minor edits have weight 0.25 (75% reduction)\n",
    "\n",
    "3. **Time Decay**: $e^{-\\lambda (t_\\text{cutoff}-t_r)}$\n",
    "   - Exponential decay based on time difference from cutoff\n",
    "   - $\\lambda = \\ln(2)/30$ gives a 30-day half-life\n",
    "   - More recent interactions have higher weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acbcf2",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The formula is implemented in the code below, where we:\n",
    "1. Calculate weights for each revision\n",
    "2. Group by user and namespace\n",
    "3. Sum the weights to get the final edge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fa96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = np.log(2) / 30\n",
    "train[\"weight\"] = (\n",
    "    np.log(train[\"textdata\"] + 1)\n",
    "    * (1 - 0.25 * train[\"minor\"])\n",
    "    * np.exp(-lamb * (train[\"timestamp\"].rsub(test_lim).dt.days))\n",
    ")\n",
    "graph_data = (\n",
    "    train.groupby([\"user\", \"namespace\"])[\"weight\"]\n",
    "    .sum()\n",
    "    .reset_index(name=\"weight\")\n",
    "    .sort_values(\"weight\", ascending=False)\n",
    ")\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542b6e2",
   "metadata": {},
   "source": [
    "### Impact of Parameters\n",
    "\n",
    "- The 30-day half-life ($\\lambda = \\ln(2)/30$) means that:\n",
    "  - A 30-day old revision has 50% of its original weight\n",
    "  - A 60-day old revision has 25% of its original weight\n",
    "  - A 90-day old revision has 12.5% of its original weight\n",
    "\n",
    "- The 0.75 reduction for minor edits means:\n",
    "  - Major edits retain full weight (multiplied by 1.0)\n",
    "  - Minor edits retain only 25% weight (multiplied by 0.25)\n",
    "\n",
    "This weighting scheme emphasizes:\n",
    "1. Substantial contributions (through word count)\n",
    "2. Major edits over minor ones\n",
    "3. Recent activity over older interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e69b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(\n",
    "    df: pd.DataFrame,\n",
    "    cutoff_date: pd.Timestamp,\n",
    "    users: list[str] | None = None,\n",
    "    time_window: float = 30,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create graph data from article talk data up to a cutoff date.\n",
    "    \n",
    "    Args:\n",
    "        df: input DataFrame containing article talk data\n",
    "        cutoff_date: timestamp to filter the data\n",
    "        users: optional list of usernames to keep in the final output\n",
    "        time_window: time window in days for exponential decay (default: 30)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with user, namespace and weight columns\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: if required columns are missing or time_window is not positive\n",
    "    \"\"\"\n",
    "    required_cols = {\"user\", \"namespace\", \"timestamp\", \"minor\", \"textdata\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        missing = required_cols - set(df.columns)\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "        \n",
    "    if time_window <= 0:\n",
    "        raise ValueError(\"time_window must be positive\")\n",
    "\n",
    "    # Filter data up to cutoff date\n",
    "    filtered_data = df[df[\"timestamp\"] <= cutoff_date].copy()\n",
    "    \n",
    "    if users is not None:\n",
    "        filtered_data = filtered_data[filtered_data[\"user\"].isin(users)]\n",
    "\n",
    "    # Calculate weights using exponential decay\n",
    "    lamb = np.log(2) / time_window\n",
    "    filtered_data[\"weight\"] = (\n",
    "        np.log(filtered_data[\"textdata\"] + 1)\n",
    "        * (1 - 0.25 * filtered_data[\"minor\"])\n",
    "        * np.exp(-lamb * (filtered_data[\"timestamp\"].rsub(cutoff_date).dt.days))\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        filtered_data.groupby([\"user\", \"namespace\"], observed=True)[\"weight\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"weight\")\n",
    "        .sort_values(\"weight\", ascending=False)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb1fb1",
   "metadata": {},
   "source": [
    "## Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = create_graph_data(\n",
    "    train,\n",
    "    cutoff_date=test_lim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c751e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite_graph_from_edgelist(\n",
    "    edgelist: pd.DataFrame,\n",
    "    source: str = \"namespace\",\n",
    "    target: str = \"user\",\n",
    "    weight: str = \"weight\",\n",
    "    B: nx.DiGraph | None = None,\n",
    ") -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Create a bipartite graph from an edge list DataFrame efficiently.\n",
    "\n",
    "    Args:\n",
    "        edgelist: DataFrame containing the edge list with user, namespace and weight columns\n",
    "        source: name of the source column (default: 'namespace')\n",
    "        target: name of the target column (default: 'user')\n",
    "        weight: name of the weight column (default: 'weight')\n",
    "\n",
    "    Returns:\n",
    "        A NetworkX bipartite graph\n",
    "    \"\"\"\n",
    "    if B is None:\n",
    "        # Create a new directed graph\n",
    "        B = nx.DiGraph()\n",
    "\n",
    "    # Add nodes in bulk operations\n",
    "    B.add_nodes_from(edgelist[source].unique(), bipartite=0)\n",
    "    B.add_nodes_from(edgelist[target].unique(), bipartite=1)\n",
    "\n",
    "    # Add edges in bulk operation\n",
    "    edges = list(\n",
    "        zip(\n",
    "            edgelist[source],\n",
    "            edgelist[target],\n",
    "            edgelist[weight],\n",
    "        )\n",
    "    )\n",
    "    B.add_weighted_edges_from(edges)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = bipartite_graph_from_edgelist(graph_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f326af",
   "metadata": {},
   "source": [
    "## Interactive Graph Visualization\n",
    "\n",
    "We use pyvis to create interactive visualizations of our Wikipedia interaction networks. The visualization features:\n",
    "\n",
    "### Node Properties:\n",
    "- Size proportional to node degree (larger = more connections)\n",
    "- Color coding for bipartite sets:\n",
    "  - Blue: Article namespaces\n",
    "  - Red: Users\n",
    "\n",
    "### Edge Properties:\n",
    "- Directed edges showing edit relationships\n",
    "- Edge weights reflecting interaction strength\n",
    "\n",
    "### Interactive Features:\n",
    "- Zoom and pan\n",
    "- Node dragging\n",
    "- Physics simulation controls\n",
    "- Node/edge filtering options\n",
    "\n",
    "This visualization helps us understand:\n",
    "- Community structure\n",
    "- Key users and articles\n",
    "- Interaction patterns\n",
    "- Network density and connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(graph: nx.DiGraph):\n",
    "    \"\"\"\n",
    "    Create an interactive visualization of the graph using pyvis.\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph to visualize\n",
    "        \n",
    "    The visualization will:\n",
    "    - Use a dark theme with white text\n",
    "    - Show interactive controls for physics, nodes, and edges\n",
    "    - Scale node sizes based on degree\n",
    "    - Color nodes based on bipartite sets\n",
    "    \"\"\"\n",
    "    # Create network with custom settings\n",
    "    network = Network(\n",
    "        height=\"1080px\", \n",
    "        directed=True,\n",
    "        bgcolor=\"#222222\",\n",
    "        font_color=\"white\",\n",
    "        select_menu=True\n",
    "    )\n",
    "    \n",
    "    # Enable interactive controls\n",
    "    network.show_buttons(filter_=[\"physics\", \"nodes\", \"edges\"])\n",
    "    \n",
    "    # Add nodes with different colors for bipartite sets\n",
    "    for node, nodedata in tqdm(graph.nodes(data='bipartite', default=0), desc=\"Adding nodes\"):\n",
    "        color = \"#4287f5\" if nodedata == 0 else \"#f54242\"\n",
    "        degree = graph.out_degree(node) if nodedata == 0 else graph.in_degree(node)\n",
    "        network.add_node(\n",
    "            node,\n",
    "            color=color,\n",
    "            size=10 + degree,\n",
    "            title=f\"Node: {node}\\nDegree: {degree}\"\n",
    "        )\n",
    "    \n",
    "    # Add edges with weights\n",
    "    for source, target, weight in tqdm(graph.edges.data('weight', default=1.0), desc=\"Adding edges\"):\n",
    "        network.add_edge(source, target, value=weight)\n",
    "    \n",
    "    # Show the graph\n",
    "    network.show(\"./graphs/vis/user-article.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16313816",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35b5e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_graph = graph.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be859084",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_graph = bipartite.projected_graph(rev_graph, graph_data[\"user\"].unique()).to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee1e6d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25286, 81927)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_graph.remove_nodes_from(\n",
    "    [node for node, degree in user_graph.degree() if degree == 0]\n",
    ")\n",
    "user_graph.number_of_nodes(), user_graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065656be",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba36cd8",
   "metadata": {},
   "source": [
    "## Graph Weight Function: Implementation Details\n",
    "\n",
    "The weight function implementation includes several optimizations for performance:\n",
    "\n",
    "1. **Efficient Data Structures**:\n",
    "   - Pre-computed weight dictionary to avoid redundant lookups\n",
    "   - Set operations for neighborhood comparisons\n",
    "   - Vectorized numpy operations where possible\n",
    "\n",
    "2. **Modular Design**:\n",
    "   - Separate functions for each metric type\n",
    "   - Reusable components for different graph analyses\n",
    "   - Clear separation of concerns\n",
    "\n",
    "3. **Memory Optimization**:\n",
    "   - Single-pass computations where possible\n",
    "   - Early returns for edge cases\n",
    "   - Minimal data duplication\n",
    "\n",
    "The implementation balances computational efficiency with memory usage, making it suitable for large-scale network analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0167a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basic_weight(common_neighbors: set, nbr_weights: dict) -> float:\n",
    "    \"\"\"Compute the basic weight between two nodes based on common neighbors.\"\"\"\n",
    "    if not common_neighbors:  # Early return for empty set\n",
    "        return 0.0\n",
    "        \n",
    "    # Pre-calculate values for common neighbors\n",
    "    weights = [(nbr_weights[nbr]['u'] + nbr_weights[nbr]['v'], \n",
    "               nbr_weights[nbr]['total']) for nbr in common_neighbors]\n",
    "    \n",
    "    # Use numpy for faster array operations\n",
    "    weights = np.array(weights)\n",
    "    return np.sum(weights[:, 0]) / np.sum(weights[:, 1]) if weights.size > 0 else 0.0\n",
    "\n",
    "def compute_weighted_jaccard(all_neighbors: set, nbr_weights: dict) -> float:\n",
    "    \"\"\"Compute the weighted Jaccard similarity between two nodes.\"\"\"\n",
    "    if not all_neighbors:  # Early return for empty set\n",
    "        return 0.0\n",
    "    \n",
    "    # Pre-calculate all values in single pass\n",
    "    weights = [(nbr_weights[nbr]['u'], nbr_weights[nbr]['v']) for nbr in all_neighbors]\n",
    "    \n",
    "    # Convert to numpy array for faster operations\n",
    "    weights = np.array(weights)\n",
    "    if weights.size == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    intersection = np.minimum(weights[:, 0], weights[:, 1]).sum()\n",
    "    union = np.maximum(weights[:, 0], weights[:, 1]).sum()\n",
    "    \n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "def compute_entropy_and_mutual_info(all_neighbors: set, nbr_weights: dict, total_weight: float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute both participation entropy and mutual information efficiently.\n",
    "    \n",
    "    Args:\n",
    "        all_neighbors: set of all neighbors for both nodes\n",
    "        nbr_weights: dictionary containing edge weights for each neighbor\n",
    "        total_weight: sum of weights for both nodes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (participation_entropy, mutual_information)\n",
    "    \"\"\"\n",
    "    joint_entropy = 0.0\n",
    "    mutual_info = 0.0\n",
    "    \n",
    "    for nbr in all_neighbors:\n",
    "        nbr_total = nbr_weights[nbr]['total']\n",
    "        p_u = nbr_weights[nbr]['u'] / nbr_total if nbr_total > 0 else 0\n",
    "        p_v = nbr_weights[nbr]['v'] / nbr_total if nbr_total > 0 else 0\n",
    "        \n",
    "        # Compute entropy\n",
    "        if p_u > 0:\n",
    "            joint_entropy += -p_u * np.log2(p_u)\n",
    "        if p_v > 0:\n",
    "            joint_entropy += -p_v * np.log2(p_v)\n",
    "            \n",
    "        # Compute mutual information\n",
    "        if total_weight > 0:\n",
    "            joint_prob = (nbr_weights[nbr]['u'] + nbr_weights[nbr]['v']) / total_weight\n",
    "            if p_u > 0 and p_v > 0 and joint_prob > 0:\n",
    "                mutual_info += joint_prob * np.log2(joint_prob / (p_u * p_v))\n",
    "    \n",
    "    return joint_entropy, mutual_info\n",
    "\n",
    "def weight_function(B: nx.DiGraph, u: str, v: str) -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate multiple metrics between two users in a user-article bipartite graph.\n",
    "    \n",
    "    Args:\n",
    "        B: bipartite graph\n",
    "        u: first user\n",
    "        v: second user\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (basic_weight, weighted_jaccard, participation_entropy, mutual_info)\n",
    "    \"\"\"\n",
    "    # Get neighborhoods once\n",
    "    unbrs = set(B.predecessors(u))\n",
    "    vnbrs = set(B.predecessors(v))\n",
    "    common_neighbors = unbrs & vnbrs\n",
    "    all_neighbors = unbrs | vnbrs\n",
    "    \n",
    "    # Pre-compute all weights in a single dictionary\n",
    "    nbr_weights = {\n",
    "        nbr: {\n",
    "            'u': B.edges[nbr, u]['weight'] if B.has_edge(nbr, u) else 0.0,\n",
    "            'v': B.edges[nbr, v]['weight'] if B.has_edge(nbr, v) else 0.0,\n",
    "            'total': B.out_degree(nbr, weight=\"weight\")\n",
    "        }\n",
    "        for nbr in all_neighbors\n",
    "    }\n",
    "    \n",
    "    # Calculate total weight once\n",
    "    total_weight = B.in_degree(u, weight=\"weight\") + B.in_degree(v, weight=\"weight\")\n",
    "    \n",
    "    # Compute all metrics using the pre-computed weights\n",
    "    basic_weight = compute_basic_weight(common_neighbors, nbr_weights)\n",
    "    weighted_jaccard = compute_weighted_jaccard(all_neighbors, nbr_weights)\n",
    "    participation_entropy, mutual_info = compute_entropy_and_mutual_info(\n",
    "        all_neighbors, nbr_weights, total_weight\n",
    "    )\n",
    "    \n",
    "    return basic_weight, weighted_jaccard, participation_entropy, mutual_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76c301",
   "metadata": {},
   "source": [
    "## Feature Engineering: Graph-Based Metrics\n",
    "\n",
    "This section implements various graph-based metrics to analyze user interactions in Wikipedia. We compute several important metrics:\n",
    "\n",
    "### User-Article (Bipartite) Graph Metrics:\n",
    "1. **Basic Weight**: Normalized sum of weights for common article interactions\n",
    "2. **Weighted Jaccard**: Similarity measure based on shared article interactions\n",
    "3. **Participation Entropy**: Measures diversity of user participation\n",
    "4. **Mutual Information**: Quantifies information shared between user interaction patterns\n",
    "\n",
    "### User-User (Social) Graph Metrics:\n",
    "1. **Jaccard Index**: Measures similarity of users' neighbor sets\n",
    "2. **Adamic-Adar Index**: Weighted similarity metric favoring rare shared neighbors\n",
    "3. **Preferential Attachment**: Product of node degrees\n",
    "4. **PageRank**: Measures global importance of users in the network\n",
    "\n",
    "These metrics help us understand:\n",
    "- How similarly users interact with articles\n",
    "- The strength and nature of user relationships\n",
    "- Users' roles and importance in the Wikipedia community\n",
    "- Potential voting patterns in admin elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_article_metrics(B: nx.DiGraph, voters: list[str], candidate: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Efficiently compute weight metrics for all pairs of users using numpy vectorization.\n",
    "    \n",
    "    Args:\n",
    "        B: bipartite graph containing user-article interactions\n",
    "        usernames: list of usernames to analyze\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing weight metrics for each user pair\n",
    "    \"\"\"\n",
    "    # Filter valid users that exist in the graph\n",
    "    valid_voters = [u for u in sorted(voters) if u in B]\n",
    "    valid_candidate = candidate if candidate in B else None\n",
    "\n",
    "    if valid_candidate is None:\n",
    "        return pd.DataFrame(columns=[\"user1\", \"user2\", \"collaboration\", \"pairwise_jaccard\", \"participation_entropy\", \"mutual_information\"])\n",
    "\n",
    "    # Create user pairs using combinations\n",
    "    pairs = list(itertools.product(valid_voters, [valid_candidate]))\n",
    "    \n",
    "    if not pairs:\n",
    "        return pd.DataFrame(columns=[\"user1\", \"user2\", \"collaboration\", \"pairwise_jaccard\", \"participation_entropy\", \"mutual_information\"])\n",
    "    \n",
    "    # Pre-allocate metrics list\n",
    "    metrics = []\n",
    "    for user1, user2 in pairs:\n",
    "        # Compute metrics for each pair\n",
    "        basic_w, jaccard_w, entropy, mutual_info = weight_function(B, user1, user2)\n",
    "\n",
    "        metrics.append({\n",
    "        'user1': user1,\n",
    "        'user2': user2,\n",
    "        'collaboration': basic_w,\n",
    "        'pairwise_jaccard': jaccard_w,\n",
    "        'participation_entropy': entropy,\n",
    "        'mutual_information': mutual_info\n",
    "    })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Sort only once at the end\n",
    "    return results.sort_values('collaboration', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3772738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_user_metrics(G: nx.Graph, voters: list[str], candidate: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute graph metrics for pairs of users in a social network using vectorized operations.\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX undirected graph representing user interactions\n",
    "        usernames: List of usernames to analyze\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with metrics for each user pair:\n",
    "        - Jaccard index\n",
    "        - Adamic-Adar index\n",
    "        - Preferential attachment\n",
    "        - PageRank of each node\n",
    "        - Katz centrality of each node\n",
    "    \"\"\"\n",
    "    # Filter valid users that exist in the graph\n",
    "    valid_voters = [u for u in sorted(voters) if u in G]\n",
    "    valid_candidate = candidate if candidate in G else None\n",
    "\n",
    "    if valid_candidate is None:\n",
    "        return pd.DataFrame(columns=[\"user1\", \"user2\", \"jaccard\", \"adamic_adar\", \"pref_attachment\", \"pagerank_1\", \"pagerank_2\"])\n",
    "\n",
    "    # Create user pairs using combinations\n",
    "    pairs = list(itertools.product(valid_voters, [valid_candidate]))\n",
    "    \n",
    "    if not pairs:\n",
    "        return pd.DataFrame(columns=[\"user1\", \"user2\", \"jaccard\", \"adamic_adar\", \"pref_attachment\", \"pagerank_1\", \"pagerank_2\"])\n",
    "\n",
    "    # Pre-compute centrality measures\n",
    "    pagerank = nx.pagerank(G)\n",
    "    jaccard = {(u, v): p for u, v, p in nx.jaccard_coefficient(G, ebunch=pairs)}\n",
    "    adamic_adar = {(u, v): p for u, v, p in nx.adamic_adar_index(G, ebunch=pairs)}\n",
    "    pref_attach = {(u, v): p for u, v, p in nx.preferential_attachment(G, ebunch=pairs)}\n",
    "\n",
    "    metrics = []\n",
    "    for u, v in pairs:\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"user1\": u,\n",
    "                \"user2\": v,\n",
    "                \"jaccard\": jaccard[(u, v)],\n",
    "                \"adamic_adar\": adamic_adar[(u, v)],\n",
    "                \"pref_attachment\": pref_attach[(u, v)],\n",
    "                \"pagerank_1\": pagerank[u],\n",
    "                \"pagerank_2\": pagerank[v]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fb2741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_data = pd.read_csv(\"data/votes_with_election_info.csv\")\n",
    "voting_data[\"start_time\"] = pd.to_datetime(voting_data[\"start_time\"])\n",
    "voting_data.sort_values(\n",
    "    by=[\"start_time\"], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702a572",
   "metadata": {},
   "source": [
    "## Computing Election-Specific Features\n",
    "\n",
    "This section focuses on computing features for Wikipedia administrator elections. For each election:\n",
    "\n",
    "1. **Time-Based Graph Construction**:\n",
    "   - Creates a graph using data up to the election date\n",
    "   - Filters interactions with weight ≥ 1 to focus on significant interactions\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - Computes bipartite metrics between voters and the candidate\n",
    "   - Generates social network metrics from the user projection\n",
    "   - Combines both types of metrics into comprehensive feature vectors\n",
    "\n",
    "3. **Output**:\n",
    "   - Saves features for each election in separate CSV files\n",
    "   - Files named with pattern: `article_edits.{date}.{candidate}.csv`\n",
    "\n",
    "This temporal approach ensures we only use data that would have been available at the time of each election, preventing data leakage in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70da547",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_elections = voting_data[voting_data[\"start_time\"] > \"2005-08-31\"]\n",
    "\n",
    "for date in tqdm(missing_elections[\"start_time\"].unique()):\n",
    "    graph_data = create_graph_data(article_edits, cutoff_date=date)\n",
    "    graph_data = graph_data[graph_data[\"weight\"] >= 1]\n",
    "\n",
    "    rows = voting_data[voting_data[\"start_time\"] == date]\n",
    "    voters = voting_data[voting_data[\"start_time\"] == date][\"voter\"].unique()\n",
    "    candidate = rows[\"candidate\"].unique()[0]\n",
    "\n",
    "    bipartite_graph: nx.DiGraph = bipartite_graph_from_edgelist(graph_data)\n",
    "    bipartite_metrics = compute_user_article_metrics(\n",
    "        bipartite_graph, voters, candidate\n",
    "    )\n",
    "\n",
    "    user_graph = bipartite.projected_graph(bipartite_graph, graph_data[\"user\"].unique()).to_undirected()\n",
    "    monopartite_metrics = compute_user_user_metrics(\n",
    "    user_graph,\n",
    "    voters,\n",
    "    candidate\n",
    "    )\n",
    "    \n",
    "    features = pd.merge(\n",
    "        bipartite_metrics,\n",
    "        monopartite_metrics,\n",
    "        on=[\"user1\", \"user2\"])\n",
    "    \n",
    "    if features.empty:\n",
    "        continue\n",
    "\n",
    "    features.to_csv(\n",
    "        f\"data/features/article_edits.{date.strftime(f'%Y-%m-%d')}.{candidate}.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
